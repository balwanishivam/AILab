% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Week-5},
  pdfauthor={QuickFixDemos},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Week-5}
\author{QuickFixDemos}
\date{}

\begin{document}
\maketitle

Packages Used Installation

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("e1071") }
\CommentTok{\# install.packages("caTools") }
\CommentTok{\# install.packages("caret") }
\CommentTok{\# install.packages("bnlearn")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bnlearn)}
\FunctionTok{library}\NormalTok{(e1071) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Attaching package: 㤼㸱e1071㤼㸲

The following object is masked from 㤼㸱package:bnlearn㤼㸲:

    impute
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caTools) }
\FunctionTok{library}\NormalTok{(caret) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: lattice
Loading required package: ggplot2
Registered S3 method overwritten by 'data.table':
  method           from
  print.data.table     
\end{verbatim}

Lab Assignment 5

Learning Objective: Understand the graphical models for inference under
uncertainty, build Bayesian Network in R, Learn the structure and CPTs
from Data, naive Bayes classification with dependency between features.
Problem Statement: A table containing grades earned by students in
respective courses is made available to you in (codes folder)
2020\_bn\_nb\_data.txt. Q1: Consider grades earned in each of the
courses as random variables and learn the dependencies between courses.
\#\#Solution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"C:/Users/shiva/OneDrive/Desktop/Semester{-}6/AILab/Lab5/2020\_bn\_nb\_data.txt"}\NormalTok{,}\AttributeTok{head=}\ConstantTok{TRUE}\NormalTok{,}\AttributeTok{stringsAsFactors=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset}
\NormalTok{dataset\_net}\OtherTok{\textless{}{-}}\FunctionTok{hc}\NormalTok{(dataset\_grades,}\AttributeTok{score=}\StringTok{"k2"}\NormalTok{)}
\NormalTok{dataset\_net}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Bayesian network learned via Score-based methods

  model:
   [MA101][EC100|MA101][PH100|EC100][QP|EC100][EC160|QP][IT101|QP][PH160|QP][HS101|QP][IT161|IT101] 
  nodes:                                 9 
  arcs:                                  8 
    undirected arcs:                     0 
    directed arcs:                       8 
  average markov blanket size:           1.78 
  average neighbourhood size:            1.78 
  average branching factor:              0.89 

  learning algorithm:                    Hill-Climbing 
  score:                                 Cooper & Herskovits' K2 
  tests used in the learning procedure:  168 
  optimized:                             TRUE 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(dataset\_net)}
\end{Highlighting}
\end{Shaded}

Q2: Using the data, learn the CPTs for each course node.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_net\_bn\_fit }\OtherTok{\textless{}{-}} \FunctionTok{bn.fit}\NormalTok{(dataset\_net, dataset\_grades )}
\FunctionTok{print}\NormalTok{(dataset\_net\_bn\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Bayesian network parameters

  Parameters of node EC100 (multinomial distribution)

Conditional probability table:
 
     MA101
EC100         AA         AB         BB         BC         CC         CD         DD          F
   AA 0.75000000 0.07692308 0.03846154 0.01851852 0.00000000 0.00000000 0.00000000 0.00000000
   AB 0.00000000 0.46153846 0.25000000 0.05555556 0.00000000 0.00000000 0.00000000 0.00000000
   BB 0.25000000 0.23076923 0.32692308 0.22222222 0.04081633 0.00000000 0.00000000 0.00000000
   BC 0.00000000 0.15384615 0.28846154 0.27777778 0.32653061 0.00000000 0.00000000 0.00000000
   CC 0.00000000 0.07692308 0.09615385 0.24074074 0.32653061 0.04166667 0.00000000 0.00000000
   CD 0.00000000 0.00000000 0.00000000 0.12962963 0.26530612 0.33333333 0.04761905 0.00000000
   DD 0.00000000 0.00000000 0.00000000 0.03703704 0.04081633 0.50000000 0.19047619 0.00000000
   F  0.00000000 0.00000000 0.00000000 0.01851852 0.00000000 0.12500000 0.76190476 1.00000000

  Parameters of node EC160 (multinomial distribution)

Conditional probability table:
 
     QP
EC160          n          y
   AA 0.00000000 0.07500000
   AB 0.00000000 0.10000000
   BB 0.01388889 0.18750000
   BC 0.01388889 0.36250000
   CC 0.15277778 0.22500000
   CD 0.44444444 0.03125000
   DD 0.26388889 0.01875000
   F  0.11111111 0.00000000

  Parameters of node IT101 (multinomial distribution)

Conditional probability table:
 
     QP
IT101          n          y
   AA 0.00000000 0.07500000
   AB 0.00000000 0.15625000
   BB 0.04166667 0.19375000
   BC 0.02777778 0.29375000
   CC 0.13888889 0.20000000
   CD 0.30555556 0.08125000
   DD 0.31944444 0.00000000
   F  0.16666667 0.00000000

  Parameters of node IT161 (multinomial distribution)

Conditional probability table:
 
     IT101
IT161         AA         AB         BB         BC         CC         CD         DD          F
   AA 0.58333333 0.24000000 0.14705882 0.04081633 0.00000000 0.00000000 0.00000000 0.00000000
   AB 0.16666667 0.40000000 0.29411765 0.02040816 0.04761905 0.00000000 0.00000000 0.00000000
   BB 0.16666667 0.24000000 0.32352941 0.20408163 0.11904762 0.02857143 0.00000000 0.00000000
   BC 0.08333333 0.04000000 0.20588235 0.36734694 0.38095238 0.17142857 0.00000000 0.00000000
   CC 0.00000000 0.04000000 0.00000000 0.24489796 0.33333333 0.31428571 0.08695652 0.16666667
   CD 0.00000000 0.04000000 0.02941176 0.10204082 0.09523810 0.31428571 0.52173913 0.08333333
   DD 0.00000000 0.00000000 0.00000000 0.02040816 0.02380952 0.14285714 0.39130435 0.58333333
   F  0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.02857143 0.00000000 0.16666667

  Parameters of node MA101 (multinomial distribution)

Conditional probability table:
         AA         AB         BB         BC         CC         CD         DD          F 
0.01724138 0.05603448 0.22413793 0.23275862 0.21120690 0.10344828 0.09051724 0.06465517 

  Parameters of node PH100 (multinomial distribution)

Conditional probability table:
 
     EC100
PH100         AA         AB         BB         BC         CC         CD         DD          F
   AA 0.71428571 0.40909091 0.22857143 0.08333333 0.00000000 0.00000000 0.00000000 0.00000000
   AB 0.14285714 0.31818182 0.20000000 0.18750000 0.05555556 0.00000000 0.00000000 0.00000000
   BB 0.00000000 0.18181818 0.31428571 0.29166667 0.13888889 0.03448276 0.05000000 0.00000000
   BC 0.14285714 0.04545455 0.14285714 0.22916667 0.33333333 0.13793103 0.00000000 0.00000000
   CC 0.00000000 0.04545455 0.11428571 0.18750000 0.25000000 0.41379310 0.20000000 0.02857143
   CD 0.00000000 0.00000000 0.00000000 0.02083333 0.19444444 0.31034483 0.45000000 0.11428571
   DD 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778 0.10344828 0.20000000 0.45714286
   F  0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.10000000 0.40000000

  Parameters of node PH160 (multinomial distribution)

Conditional probability table:
 
     QP
PH160          n          y
   AA 0.05555556 0.14375000
   AB 0.09722222 0.15625000
   BB 0.02777778 0.17500000
   BC 0.18055556 0.34375000
   CC 0.29166667 0.13750000
   CD 0.19444444 0.04375000
   DD 0.12500000 0.00000000
   F  0.02777778 0.00000000

  Parameters of node HS101 (multinomial distribution)

Conditional probability table:
 
     QP
HS101          n          y
   AA 0.00000000 0.26250000
   AB 0.00000000 0.21250000
   BB 0.05555556 0.22500000
   BC 0.12500000 0.16875000
   CC 0.18055556 0.08125000
   CD 0.19444444 0.03750000
   DD 0.37500000 0.01250000
   F  0.06944444 0.00000000

  Parameters of node QP (multinomial distribution)

Conditional probability table:
 
   EC100
QP         AA        AB        BB        BC        CC        CD        DD         F
  n 0.0000000 0.0000000 0.0000000 0.0000000 0.1388889 0.4482759 0.9500000 1.0000000
  y 1.0000000 1.0000000 1.0000000 1.0000000 0.8611111 0.5517241 0.0500000 0.0000000
\end{verbatim}

Q3: What grade will a student get in PH100 if he earns DD in EC100, CC
in IT101 and CD in MA101.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grade\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"AA"}\NormalTok{,}\StringTok{"AB"}\NormalTok{,}\StringTok{"BB"}\NormalTok{,}\StringTok{"BC"}\NormalTok{,}\StringTok{"CC"}\NormalTok{,}\StringTok{"CD"}\NormalTok{,}\StringTok{"DD"}\NormalTok{,}\StringTok{"F"}\NormalTok{)}
\NormalTok{probability }\OtherTok{\textless{}{-}} \FloatTok{0.0}
\NormalTok{result}\OtherTok{=}\StringTok{""}
\ControlFlowTok{for}\NormalTok{(grade }\ControlFlowTok{in}\NormalTok{ grade\_list) \{}
\NormalTok{  prob }\OtherTok{\textless{}{-}} \FunctionTok{cpquery}\NormalTok{(dataset\_net\_bn\_fit, }\AttributeTok{event =}\NormalTok{ (PH100}\SpecialCharTok{==}\NormalTok{ grade), }\AttributeTok{evidence =}\NormalTok{ (EC100}\SpecialCharTok{==}\StringTok{"DD"} \SpecialCharTok{\&}\NormalTok{ IT101}\SpecialCharTok{==}\StringTok{"CC"} \SpecialCharTok{\&}\NormalTok{ MA101}\SpecialCharTok{==}\StringTok{"CD"}\NormalTok{))}
  \ControlFlowTok{if}\NormalTok{(probability}\SpecialCharTok{\textless{}}\NormalTok{prob)\{}
\NormalTok{    probability}\OtherTok{=}\NormalTok{prob;}
\NormalTok{    result}\OtherTok{=}\NormalTok{grade}
\NormalTok{  \}}
\NormalTok{\}}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The max probability of resultant grade is \%f"}\NormalTok{,probability)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "The max probability of resultant grade is 0.434599"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The max grade obtained with given ecidence is \%s "}\NormalTok{,result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "The max grade obtained with given ecidence is CD "
\end{verbatim}

Q4(a): The last column in the data file indicates whether a student
qualifies for an internship program or not. From the given data, take 70
percent data for training and build a naive Bayes classifier
(considering that the grades earned in different courses are independent
of each other) which takes in the student's performance and returns the
qualification status with a probability. Test your classifier on the
remaining 30 percent data. Repeat this experiment for 20 random
selection of training and testing data. Report results about the
accuracy of your classifier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{sample.split}\NormalTok{(dataset\_grades, }\AttributeTok{SplitRatio =} \FloatTok{0.7}\NormalTok{) }
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"TRUE"}\NormalTok{) }
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"FALSE"}\NormalTok{) }
\NormalTok{naive\_bayes\_classifier}\OtherTok{\textless{}{-}} \FunctionTok{naiveBayes}\NormalTok{(QP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{y\_train}\OtherTok{=}\FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier, }\AttributeTok{newdata =}\NormalTok{ train)}
\NormalTok{y\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier, }\AttributeTok{newdata =}\NormalTok{ test)}
\NormalTok{cm\_train}\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{QP, y\_train)}
\NormalTok{accuracy\_train }\OtherTok{=}\NormalTok{ (cm\_train[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_train[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_train)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Train Accuracy"} \OtherTok{=}\NormalTok{accuracy\_train), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Train Accuracy
[1,]         0.9935
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{QP, y\_prediction)}
\NormalTok{accuracy\_test }\OtherTok{=}\NormalTok{ (cm\_test[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_test[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Test Accuracy"} \OtherTok{=}\NormalTok{accuracy\_test), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Test Accuracy
[1,]        0.9351
\end{verbatim}

Q4(b): Repeat this experiment for 20 random selection of training and
testing data. Report results about the accuracy of your classifier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset}
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset\_grades[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(dataset\_grades), }\DecValTok{20}\NormalTok{), ]}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{sample.split}\NormalTok{(dataset\_grades, }\AttributeTok{SplitRatio =} \FloatTok{0.7}\NormalTok{) }
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"TRUE"}\NormalTok{) }
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"FALSE"}\NormalTok{)}
\NormalTok{naive\_bayes\_classifier}\OtherTok{\textless{}{-}} \FunctionTok{naiveBayes}\NormalTok{(QP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{y\_train}\OtherTok{=}\FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier, }\AttributeTok{newdata =}\NormalTok{ train)}
\NormalTok{y\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier, }\AttributeTok{newdata =}\NormalTok{ test)}
\NormalTok{cm\_train}\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{QP, y\_train)}
\NormalTok{accuracy\_train }\OtherTok{=}\NormalTok{ (cm\_train[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_train[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_train)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Train Accuracy"} \OtherTok{=}\NormalTok{accuracy\_train), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Train Accuracy
[1,]              1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{QP, y\_prediction)}
\NormalTok{accuracy\_test }\OtherTok{=}\NormalTok{ (cm\_test[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_test[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Test Accuracy"} \OtherTok{=}\NormalTok{accuracy\_test), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Test Accuracy
[1,]        0.8333
\end{verbatim}

Q5(a): Repeat 4, considering that the grades earned in different courses
may be dependent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{sample.split}\NormalTok{(dataset\_grades, }\AttributeTok{SplitRatio =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"TRUE"}\NormalTok{)}
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"FALSE"}\NormalTok{)}
\NormalTok{train.hc}\OtherTok{=}\FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{hc}\NormalTok{(train, }\AttributeTok{score=}\StringTok{"k2"}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(train.hc)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{naive\_bayes\_classifier}\OtherTok{\textless{}{-}} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{bn.fit}\NormalTok{(train.hc, train))}
\NormalTok{y\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier,}\AttributeTok{node=}\StringTok{"QP"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{y\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier,}\AttributeTok{node=}\StringTok{"QP"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ test)}
\NormalTok{cm\_train}\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{QP, y\_train)}
\NormalTok{accuracy\_train }\OtherTok{=}\NormalTok{ (cm\_train[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_train[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_train)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Train Accuracy"} \OtherTok{=}\NormalTok{accuracy\_train), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Train Accuracy
[1,]         0.9091
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{QP, y\_prediction)}
\NormalTok{accuracy\_test }\OtherTok{=}\NormalTok{ (cm\_test[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_test[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Test Accuracy"} \OtherTok{=}\NormalTok{accuracy\_test), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Test Accuracy
[1,]        0.9359
\end{verbatim}

Q5(b): Repeat this experiment for 20 random selection of training and
testing data. Report results about the accuracy of your classifier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset}
\NormalTok{dataset\_grades}\OtherTok{=}\NormalTok{dataset\_grades[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(dataset\_grades), }\DecValTok{20}\NormalTok{), ]}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{sample.split}\NormalTok{(dataset\_grades, }\AttributeTok{SplitRatio =} \FloatTok{0.7}\NormalTok{) }
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"TRUE"}\NormalTok{) }
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dataset\_grades, split }\SpecialCharTok{==} \StringTok{"FALSE"}\NormalTok{)}
\NormalTok{train.hc}\OtherTok{=}\FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{hc}\NormalTok{(train, }\AttributeTok{score=}\StringTok{"k2"}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(train.hc)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{naive\_bayes\_classifier}\OtherTok{\textless{}{-}} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{bn.fit}\NormalTok{(train.hc, train))}
\NormalTok{y\_train }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier,}\AttributeTok{node=}\StringTok{"QP"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{y\_prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(naive\_bayes\_classifier,}\AttributeTok{node=}\StringTok{"QP"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ test)}
\NormalTok{cm\_train}\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{QP, y\_train)}
\NormalTok{accuracy\_train }\OtherTok{=}\NormalTok{ (cm\_train[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_train[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_train)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Train Accuracy"} \OtherTok{=}\NormalTok{accuracy\_train), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Train Accuracy
[1,]         0.8571
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm\_test }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{QP, y\_prediction)}
\NormalTok{accuracy\_test }\OtherTok{=}\NormalTok{ (cm\_test[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\NormalTok{cm\_test[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(cm\_test)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"Test Accuracy"} \OtherTok{=}\NormalTok{accuracy\_test), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Test Accuracy
[1,]        0.6667
\end{verbatim}

\end{document}
